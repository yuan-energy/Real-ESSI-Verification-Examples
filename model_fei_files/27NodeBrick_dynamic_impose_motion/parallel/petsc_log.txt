************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.376e-02      1.00065   5.374e-02
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                8.867e+04      1.22124   8.196e+04  2.459e+05
Flops/sec:            1.649e+06      1.22047   1.525e+06  4.576e+06
MPI Messages:         1.000e+01      1.42857   8.167e+00  2.450e+01
MPI Message Lengths:  6.256e+03      2.22317   5.131e+02  1.257e+04
MPI Reductions:       1.800e+01      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.3735e-02 100.0%  2.4588e+05 100.0%  2.450e+01 100.0%  5.131e+02      100.0%  1.700e+01  94.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve               2 1.0 6.9141e-06 1.2 8.53e+03 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  3433
MatLUFactorNum         2 1.0 8.1062e-05 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   0 90  0  0  0  2731
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 5.5075e-05 1.1 0.00e+00 0.0 1.2e+01 1.0e+03 2.0e+00  0  0 49 96 11   0  0 49 96 12     0
MatAssemblyEnd         2 1.0 2.2101e-04 1.0 0.00e+00 0.0 1.2e+01 3.8e+01 9.0e+00  0  0 49  4 50   0  0 49  4 53     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         2 1.0 6.6996e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm                2 1.0 2.8133e-05 3.2 2.58e+02 1.2 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  6   0  0  0  0  6    26
VecCopy                2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 6 1.0 2.8610e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           2 1.0 2.8133e-05 3.2 2.58e+02 1.2 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  6   0  0  0  0  6    26
KSPSetUp               4 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               2 1.0 2.8801e-04 1.0 8.87e+04 1.2 0.0e+00 0.0e+00 1.0e+00  1100  0  0  6   1100  0  0  6   854
PCSetUp                4 1.0 2.3508e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   0 90  0  0  0   942
PCSetUpOnBlocks        2 1.0 1.8406e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   0 90  0  0  0  1203
PCApply                2 1.0 1.5020e-05 1.1 8.53e+03 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  1580
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.251e-02      1.00002   6.251e-02
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.773e+05      1.22124   1.639e+05  4.918e+05
Flops/sec:            2.837e+06      1.22126   2.622e+06  7.867e+06
MPI Messages:         1.700e+01      1.54545   1.333e+01  4.000e+01
MPI Message Lengths:  1.231e+04      2.23611   6.171e+02  2.468e+04
MPI Reductions:       2.049e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.7946e-02  92.7%  4.9177e+05 100.0%  3.700e+01  92.5%  6.171e+02      100.0%  2.200e+01   1.1% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve               4 1.0 1.2875e-05 1.2 1.71e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  3687
MatLUFactorNum         4 1.0 1.5807e-04 1.2 1.60e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   0 90  0  0  0  2801
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       4 1.0 8.4162e-05 1.1 0.00e+00 0.0 2.4e+01 1.0e+03 4.0e+00  0  0 60 98  0   0  0 65 98 18     0
MatAssemblyEnd         4 1.0 2.6608e-04 1.0 0.00e+00 0.0 1.2e+01 3.8e+01 1.1e+01  0  0 30  2  1   0  0 32  2 50     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         4 1.0 6.9857e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm                4 1.0 4.1008e-05 2.5 5.16e+02 1.2 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  9    36
VecCopy                4 1.0 2.1458e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                10 1.0 4.0531e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           4 1.0 4.1008e-05 2.3 5.16e+02 1.2 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  9    36
KSPSetUp               6 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               4 1.0 3.9005e-04 1.0 1.77e+05 1.2 0.0e+00 0.0e+00 2.0e+00  1100  0  0  0   1100  0  0  9  1261
PCSetUp                8 1.0 3.1018e-04 1.1 1.60e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   1 90  0  0  0  1428
PCSetUpOnBlocks        4 1.0 1.8501e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 45  0  0  0   0 45  0  0  0  1197
PCApply                4 1.0 1.0610e-04 1.1 9.69e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 55  0  0  0   0 55  0  0  0  2534
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.57356e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.083e-02      1.00001   7.083e-02
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.660e+05      1.22124   2.459e+05  7.377e+05
Flops/sec:            3.756e+06      1.22126   3.472e+06  1.042e+07
MPI Messages:         2.400e+01      1.60000   1.850e+01  5.550e+01
MPI Message Lengths:  1.837e+04      2.24055   6.630e+02  3.680e+04
MPI Reductions:       4.080e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.1956e-02  87.5%  7.3766e+05 100.0%  4.950e+01  89.2%  6.630e+02      100.0%  2.700e+01   0.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve               6 1.0 1.7881e-05 1.1 2.56e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  3982
MatLUFactorNum         6 1.0 2.3007e-04 1.2 2.40e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   0 90  0  0  0  2887
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       6 1.0 1.0419e-04 1.2 0.00e+00 0.0 3.6e+01 1.0e+03 6.0e+00  0  0 65 99  0   0  0 73 99 22     0
MatAssemblyEnd         6 1.0 3.0494e-04 1.0 0.00e+00 0.0 1.2e+01 3.8e+01 1.3e+01  0  0 22  1  0   0  0 24  1 48     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         6 1.0 7.1764e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm                6 1.0 5.1975e-05 2.0 7.74e+02 1.2 0.0e+00 0.0e+00 3.0e+00  0  0  0  0  0   0  0  0  0 11    43
VecCopy                6 1.0 2.1458e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                14 1.0 4.0531e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           6 1.0 5.3167e-05 2.0 7.74e+02 1.2 0.0e+00 0.0e+00 3.0e+00  0  0  0  0  0   0  0  0  0 11    42
KSPSetUp               8 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               6 1.0 4.8518e-04 1.0 2.66e+05 1.2 0.0e+00 0.0e+00 3.0e+00  1100  0  0  0   1100  0  0 11  1520
PCSetUp               12 1.0 3.7909e-04 1.1 2.40e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  1752
PCSetUpOnBlocks        6 1.0 1.8501e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0  1197
PCApply                6 1.0 1.9121e-04 1.1 1.85e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 70  0  0  0   0 70  0  0  0  2688
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.897e-02      1.00001   7.897e-02
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.547e+05      1.22124   3.278e+05  9.835e+05
Flops/sec:            4.491e+06      1.22126   4.151e+06  1.245e+07
MPI Messages:         3.100e+01      1.63158   2.367e+01  7.100e+01
MPI Message Lengths:  2.442e+04      2.24279   6.889e+02  4.891e+04
MPI Reductions:       6.111e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.5865e-02  83.4%  9.8354e+05 100.0%  6.200e+01  87.3%  6.889e+02      100.0%  3.200e+01   0.5% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve               8 1.0 2.2888e-05 1.1 3.41e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4148
MatLUFactorNum         8 1.0 2.9802e-04 1.1 3.20e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   0 90  0  0  0  2972
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       8 1.0 1.2302e-04 1.2 0.00e+00 0.0 4.8e+01 1.0e+03 8.0e+00  0  0 68 99  0   0  0 77 99 25     0
MatAssemblyEnd         8 1.0 3.5095e-04 1.0 0.00e+00 0.0 1.2e+01 3.8e+01 1.5e+01  0  0 17  1  0   1  0 19  1 47     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         8 1.0 7.4625e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm                8 1.0 6.1989e-05 2.1 1.03e+03 1.2 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0 12    48
VecCopy                8 1.0 2.8610e-06 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                18 1.0 5.2452e-06 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           8 1.0 6.3181e-05 2.1 1.03e+03 1.2 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0 12    47
KSPSetUp              10 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               8 1.0 5.7936e-04 1.0 3.55e+05 1.2 0.0e+00 0.0e+00 4.0e+00  1100  0  0  0   1100  0  0 12  1698
PCSetUp               16 1.0 4.5085e-04 1.1 3.20e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  1964
PCSetUpOnBlocks        8 1.0 1.8620e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 23  0  0  0   0 23  0  0  0  1189
PCApply                8 1.0 2.7227e-04 1.1 2.74e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 77  0  0  0   0 77  0  0  0  2788
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           8.694e-02      1.00001   8.694e-02
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                4.433e+05      1.22124   4.098e+05  1.229e+06
Flops/sec:            5.099e+06      1.22126   4.714e+06  1.414e+07
MPI Messages:         3.800e+01      1.65217   2.883e+01  8.650e+01
MPI Message Lengths:  3.048e+04      2.24415   7.055e+02  6.103e+04
MPI Reductions:       8.142e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.9855e-02  80.3%  1.2294e+06 100.0%  7.450e+01  86.1%  7.055e+02      100.0%  3.700e+01   0.5% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              10 1.0 2.7895e-05 1.1 4.26e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4254
MatLUFactorNum        10 1.0 3.7503e-04 1.1 3.99e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   1 90  0  0  0  2952
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      10 1.0 1.4305e-04 1.2 0.00e+00 0.0 6.0e+01 1.0e+03 1.0e+01  0  0 69 99  0   0  0 81 99 27     0
MatAssemblyEnd        10 1.0 3.8695e-04 1.0 0.00e+00 0.0 1.2e+01 3.8e+01 1.7e+01  0  0 14  1  0   1  0 16  1 46     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        10 1.0 7.7724e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               10 1.0 7.4148e-05 2.0 1.29e+03 1.2 0.0e+00 0.0e+00 5.0e+00  0  0  0  0  0   0  0  0  0 14    50
VecCopy               10 1.0 3.0994e-06 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                22 1.0 6.1989e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          10 1.0 7.6294e-05 2.0 1.29e+03 1.2 0.0e+00 0.0e+00 5.0e+00  0  0  0  0  0   0  0  0  0 14    48
KSPSetUp              12 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              10 1.0 6.7830e-04 1.0 4.43e+05 1.2 0.0e+00 0.0e+00 5.0e+00  1100  0  0  0   1100  0  0 14  1813
PCSetUp               20 1.0 5.2691e-04 1.1 3.99e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2101
PCSetUpOnBlocks       10 1.0 1.8620e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 18  0  0  0   0 18  0  0  0  1189
PCApply               10 1.0 3.6120e-04 1.1 3.62e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 82  0  0  0   0 82  0  0  0  2780
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.95639e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           9.450e-02      1.00001   9.450e-02
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                5.320e+05      1.22124   4.918e+05  1.475e+06
Flops/sec:            5.630e+06      1.22126   5.204e+06  1.561e+07
MPI Messages:         4.500e+01      1.66667   3.400e+01  1.020e+02
MPI Message Lengths:  3.654e+04      2.24505   7.171e+02  7.314e+04
MPI Reductions:       1.017e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.3482e-02  77.8%  1.4753e+06 100.0%  8.700e+01  85.3%  7.171e+02      100.0%  4.200e+01   0.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              12 1.0 3.2902e-05 1.1 5.12e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4328
MatLUFactorNum        12 1.0 4.4084e-04 1.2 4.79e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   1 90  0  0  0  3014
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      12 1.0 1.6117e-04 1.2 0.00e+00 0.0 7.2e+01 1.0e+03 1.2e+01  0  0 71 99  0   0  0 83 99 29     0
MatAssemblyEnd        12 1.0 4.2200e-04 1.0 0.00e+00 0.0 1.2e+01 3.8e+01 1.9e+01  0  0 12  1  0   1  0 14  1 45     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        12 1.0 8.0585e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               12 1.0 8.6069e-05 2.0 1.55e+03 1.2 0.0e+00 0.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0 14    51
VecCopy               12 1.0 4.0531e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                26 1.0 7.1526e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          12 1.0 8.9169e-05 2.0 1.55e+03 1.2 0.0e+00 0.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0 14    50
KSPSetUp              14 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              12 1.0 7.6628e-04 1.0 5.32e+05 1.2 0.0e+00 0.0e+00 6.0e+00  1100  0  0  0   1100  0  0 14  1925
PCSetUp               24 1.0 5.9271e-04 1.1 4.79e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2241
PCSetUpOnBlocks       12 1.0 1.8620e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0  1189
PCApply               12 1.0 4.4036e-04 1.1 4.51e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 85  0  0  0   1 85  0  0  0  2837
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.19209e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.019e-01      1.00001   1.019e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                6.207e+05      1.22124   5.737e+05  1.721e+06
Flops/sec:            6.091e+06      1.22126   5.631e+06  1.689e+07
MPI Messages:         5.200e+01      1.67742   3.917e+01  1.175e+02
MPI Message Lengths:  4.259e+04      2.24570   7.256e+02  8.525e+04
MPI Reductions:       1.220e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.7066e-02  75.6%  1.7212e+06 100.0%  9.950e+01  84.7%  7.256e+02      100.0%  4.700e+01   0.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              14 1.0 3.7909e-05 1.2 5.97e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4382
MatLUFactorNum        14 1.0 5.0569e-04 1.2 5.59e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   1 90  0  0  0  3065
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      14 1.0 1.8024e-04 1.2 0.00e+00 0.0 8.4e+01 1.0e+03 1.4e+01  0  0 71 99  0   0  0 84 99 30     0
MatAssemblyEnd        14 1.0 4.5705e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 2.1e+01  0  0 10  1  0   1  0 12  1 45     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        14 1.0 8.2731e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               14 1.0 1.0014e-04 1.9 1.81e+03 1.2 0.0e+00 0.0e+00 7.0e+00  0  0  0  0  0   0  0  0  0 15    52
VecCopy               14 1.0 4.0531e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                30 1.0 7.1526e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          14 1.0 1.0324e-04 2.0 1.81e+03 1.2 0.0e+00 0.0e+00 7.0e+00  0  0  0  0  0   0  0  0  0 15    50
KSPSetUp              16 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              14 1.0 8.5211e-04 1.0 6.21e+05 1.2 0.0e+00 0.0e+00 7.0e+00  1100  0  0  0   1100  0  0 15  2020
PCSetUp               28 1.0 6.5565e-04 1.1 5.59e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2364
PCSetUpOnBlocks       14 1.0 1.8620e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1189
PCApply               14 1.0 5.1856e-04 1.1 5.39e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 87  0  0  0   1 87  0  0  0  2882
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.093e-01      1.00000   1.093e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                7.093e+05      1.22124   6.557e+05  1.967e+06
Flops/sec:            6.491e+06      1.22124   6.000e+06  1.800e+07
MPI Messages:         5.900e+01      1.68571   4.433e+01  1.330e+02
MPI Message Lengths:  4.865e+04      2.24619   7.321e+02  9.737e+04
MPI Reductions:       1.424e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.0645e-02  73.8%  1.9671e+06 100.0%  1.120e+02  84.2%  7.321e+02      100.0%  5.200e+01   0.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              16 1.0 4.2915e-05 1.2 6.82e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4424
MatLUFactorNum        16 1.0 5.7077e-04 1.2 6.39e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 90  0  0  0   1 90  0  0  0  3103
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      16 1.0 1.9717e-04 1.2 0.00e+00 0.0 9.6e+01 1.0e+03 1.6e+01  0  0 72100  0   0  0 86100 31     0
MatAssemblyEnd        16 1.0 4.9090e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 2.3e+01  0  0  9  0  0   1  0 11  0 44     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        16 1.0 8.5592e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               16 1.0 1.1420e-04 2.0 2.06e+03 1.2 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0 15    52
VecCopy               16 1.0 4.0531e-06 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                34 1.0 8.3447e-06 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          16 1.0 1.1730e-04 2.1 2.06e+03 1.2 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0 15    50
KSPSetUp              18 1.0 2.0981e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              16 1.0 9.3818e-04 1.0 7.09e+05 1.2 0.0e+00 0.0e+00 8.0e+00  1100  0  0  0   1100  0  0 15  2097
PCSetUp               32 1.0 7.1740e-04 1.1 6.39e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2469
PCSetUpOnBlocks       16 1.0 1.8740e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 11  0  0  0   0 11  0  0  0  1182
PCApply               16 1.0 5.9557e-04 1.1 6.27e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 88  0  0  0   1 88  0  0  0  2921
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.19209e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.166e-01      1.00000   1.166e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                7.980e+05      1.22124   7.377e+05  2.213e+06
Flops/sec:            6.842e+06      1.22124   6.325e+06  1.897e+07
MPI Messages:         6.600e+01      1.69231   4.950e+01  1.485e+02
MPI Message Lengths:  5.470e+04      2.24657   7.373e+02  1.095e+05
MPI Reductions:       1.627e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.4227e-02  72.2%  2.2130e+06 100.0%  1.245e+02  83.8%  7.373e+02      100.0%  5.700e+01   0.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              18 1.0 4.7922e-05 1.2 7.68e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4457
MatLUFactorNum        18 1.0 6.3682e-04 1.2 7.19e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3129
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      18 1.0 2.1505e-04 1.2 0.00e+00 0.0 1.1e+02 1.0e+03 1.8e+01  0  0 73100  0   0  0 87100 32     0
MatAssemblyEnd        18 1.0 5.2381e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 2.5e+01  0  0  8  0  0   1  0 10  0 44     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 8.8453e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               18 1.0 1.2922e-04 2.2 2.32e+03 1.2 0.0e+00 0.0e+00 9.0e+00  0  0  0  0  0   0  0  0  0 16    51
VecCopy               18 1.0 5.0068e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                38 1.0 8.3447e-06 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          18 1.0 1.3232e-04 2.2 2.32e+03 1.2 0.0e+00 0.0e+00 9.0e+00  0  0  0  0  0   0  0  0  0 16    50
KSPSetUp              20 1.0 2.0981e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              18 1.0 1.0240e-03 1.0 7.98e+05 1.2 0.0e+00 0.0e+00 9.0e+00  1100  0  0  0   1100  0  0 16  2161
PCSetUp               36 1.0 7.8392e-04 1.1 7.19e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2542
PCSetUpOnBlocks       18 1.0 1.8740e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  1182
PCApply               18 1.0 6.7353e-04 1.2 7.16e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2947
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.240e-01      1.00001   1.240e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                8.867e+05      1.22124   8.196e+05  2.459e+06
Flops/sec:            7.148e+06      1.22125   6.608e+06  1.982e+07
MPI Messages:         7.300e+01      1.69767   5.467e+01  1.640e+02
MPI Message Lengths:  6.076e+04      2.24688   7.414e+02  1.216e+05
MPI Reductions:       1.830e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.7830e-02  70.8%  2.4588e+06 100.0%  1.370e+02  83.5%  7.414e+02      100.0%  6.200e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              20 1.0 5.2929e-05 1.2 8.53e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4484
MatLUFactorNum        20 1.0 7.0286e-04 1.2 7.99e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3150
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      20 1.0 2.3293e-04 1.2 0.00e+00 0.0 1.2e+02 1.0e+03 2.0e+01  0  0 73100  0   0  0 88100 32     0
MatAssemblyEnd        20 1.0 5.5790e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 2.7e+01  0  0  7  0  0   1  0  9  0 44     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        20 1.0 9.1553e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               20 1.0 1.3423e-04 1.9 2.58e+03 1.2 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0 16    55
VecCopy               20 1.0 5.0068e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                42 1.0 8.3447e-06 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          20 1.0 1.3733e-04 1.9 2.58e+03 1.2 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0 16    54
KSPSetUp              22 1.0 2.0981e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              20 1.0 1.1189e-03 1.0 8.87e+05 1.2 0.0e+00 0.0e+00 1.0e+01  1100  0  0  0   1100  0  0 16  2198
PCSetUp               40 1.0 8.5378e-04 1.1 7.99e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2593
PCSetUpOnBlocks       20 1.0 1.8740e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  9  0  0  0   0  9  0  0  0  1182
PCApply               20 1.0 7.5245e-04 1.1 8.04e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 91  0  0  0   1 91  0  0  0  2964
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.315e-01      1.00001   1.315e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                9.753e+05      1.22124   9.016e+05  2.705e+06
Flops/sec:            7.418e+06      1.22125   6.857e+06  2.057e+07
MPI Messages:         8.000e+01      1.70213   5.983e+01  1.795e+02
MPI Message Lengths:  6.682e+04      2.24712   7.449e+02  1.337e+05
MPI Reductions:       2.033e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 9.1501e-02  69.6%  2.7047e+06 100.0%  1.495e+02  83.3%  7.449e+02      100.0%  6.700e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              22 1.0 5.7936e-05 1.2 9.38e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4506
MatLUFactorNum        22 1.0 7.6866e-04 1.2 8.79e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3169
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      22 1.0 2.5392e-04 1.2 0.00e+00 0.0 1.3e+02 1.0e+03 2.2e+01  0  0 74100  0   0  0 88100 33     0
MatAssemblyEnd        22 1.0 5.9676e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 2.9e+01  0  0  7  0  0   1  0  8  0 43     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        22 1.0 9.4652e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               22 1.0 1.4830e-04 2.1 2.84e+03 1.2 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0 16    55
VecCopy               22 1.0 5.9605e-06 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                46 1.0 8.3447e-06 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          22 1.0 1.5140e-04 2.0 2.84e+03 1.2 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0 16    54
KSPSetUp              24 1.0 2.0981e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              22 1.0 1.2059e-03 1.0 9.75e+05 1.2 0.0e+00 0.0e+00 1.1e+01  1100  0  0  0   1100  0  0 16  2243
PCSetUp               44 1.0 9.2387e-04 1.1 8.79e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2636
PCSetUpOnBlocks       22 1.0 1.8740e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  8  0  0  0   0  8  0  0  0  1182
PCApply               22 1.0 8.3160e-04 1.2 8.93e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 92  0  0  0   1 92  0  0  0  2976
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.57356e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.389e-01      1.00002   1.389e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.064e+06      1.22124   9.835e+05  2.951e+06
Flops/sec:            7.659e+06      1.22127   7.080e+06  2.124e+07
MPI Messages:         8.700e+01      1.70588   6.500e+01  1.950e+02
MPI Message Lengths:  7.287e+04      2.24733   7.478e+02  1.458e+05
MPI Reductions:       2.236e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 9.5147e-02  68.5%  2.9506e+06 100.0%  1.620e+02  83.1%  7.478e+02      100.0%  7.200e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              24 1.0 6.1989e-05 1.2 1.02e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4594
MatLUFactorNum        24 1.0 8.4066e-04 1.2 9.59e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3161
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      24 1.0 2.7180e-04 1.2 0.00e+00 0.0 1.4e+02 1.0e+03 2.4e+01  0  0 74100  0   0  0 89100 33     0
MatAssemblyEnd        24 1.0 6.3157e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 3.1e+01  0  0  6  0  0   1  0  7  0 43     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        24 1.0 9.6560e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               24 1.0 1.5831e-04 2.1 3.10e+03 1.2 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  0   0  0  0  0 17    56
VecCopy               24 1.0 7.1526e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                50 1.0 9.2983e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          24 1.0 1.6236e-04 2.1 3.10e+03 1.2 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  0   0  0  0  0 17    55
KSPSetUp              26 1.0 2.0981e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              24 1.0 1.3011e-03 1.0 1.06e+06 1.2 0.0e+00 0.0e+00 1.2e+01  1100  0  0  0   1100  0  0 17  2268
PCSetUp               48 1.0 9.9993e-04 1.1 9.59e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2657
PCSetUpOnBlocks       24 1.0 1.8740e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  8  0  0  0   0  8  0  0  0  1182
PCApply               24 1.0 9.1672e-04 1.1 9.81e+05 1.2 0.0e+00 0.0e+00 0.0e+00  1 92  0  0  0   1 92  0  0  0  2967
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.62125e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.463e-01      1.00002   1.463e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.153e+06      1.22124   1.066e+06  3.197e+06
Flops/sec:            7.880e+06      1.22127   7.284e+06  2.185e+07
MPI Messages:         9.400e+01      1.70909   7.017e+01  2.105e+02
MPI Message Lengths:  7.893e+04      2.24751   7.503e+02  1.579e+05
MPI Reductions:       2.439e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 9.8766e-02  67.5%  3.1965e+06 100.0%  1.745e+02  82.9%  7.503e+02      100.0%  7.700e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              26 1.0 6.6996e-05 1.2 1.11e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4605
MatLUFactorNum        26 1.0 9.0647e-04 1.2 1.04e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3175
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      26 1.0 2.8992e-04 1.2 0.00e+00 0.0 1.6e+02 1.0e+03 2.6e+01  0  0 74100  0   0  0 89100 34     0
MatAssemblyEnd        26 1.0 6.6757e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 3.3e+01  0  0  6  0  0   1  0  7  0 43     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        26 1.0 9.9659e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               26 1.0 1.7023e-04 2.1 3.35e+03 1.2 0.0e+00 0.0e+00 1.3e+01  0  0  0  0  0   0  0  0  0 17    56
VecCopy               26 1.0 8.1062e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                54 1.0 1.0252e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          26 1.0 1.7619e-04 2.1 3.35e+03 1.2 0.0e+00 0.0e+00 1.3e+01  0  0  0  0  0   0  0  0  0 17    54
KSPSetUp              28 1.0 2.0981e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              26 1.0 1.3890e-03 1.0 1.15e+06 1.2 0.0e+00 0.0e+00 1.3e+01  1100  0  0  0   1100  0  0 17  2301
PCSetUp               52 1.0 1.0688e-03 1.1 1.04e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2693
PCSetUpOnBlocks       26 1.0 1.8835e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1176
PCApply               26 1.0 9.9468e-04 1.2 1.07e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 93  0  0  0   1 93  0  0  0  2981
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.00272e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.537e-01      1.00001   1.537e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.241e+06      1.22124   1.147e+06  3.442e+06
Flops/sec:            8.076e+06      1.22126   7.465e+06  2.240e+07
MPI Messages:         1.010e+02      1.71186   7.533e+01  2.260e+02
MPI Message Lengths:  8.498e+04      2.24766   7.524e+02  1.701e+05
MPI Reductions:       2.642e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.0240e-01  66.6%  3.4424e+06 100.0%  1.870e+02  82.7%  7.524e+02      100.0%  8.200e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              28 1.0 7.2002e-05 1.2 1.19e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4615
MatLUFactorNum        28 1.0 9.7251e-04 1.2 1.12e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3187
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      28 1.0 3.0899e-04 1.2 0.00e+00 0.0 1.7e+02 1.0e+03 2.8e+01  0  0 74100  0   0  0 90100 34     0
MatAssemblyEnd        28 1.0 7.0453e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 3.5e+01  0  0  5  0  0   1  0  6  0 43     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        28 1.0 1.0562e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               28 1.0 1.7834e-04 2.1 3.61e+03 1.2 0.0e+00 0.0e+00 1.4e+01  0  0  0  0  0   0  0  0  0 17    58
VecCopy               28 1.0 8.1062e-06 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                58 1.0 1.1206e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          28 1.0 1.8525e-04 2.2 3.61e+03 1.2 0.0e+00 0.0e+00 1.4e+01  0  0  0  0  0   0  0  0  0 17    56
KSPSetUp              30 1.0 2.0981e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              28 1.0 1.4760e-03 1.0 1.24e+06 1.2 0.0e+00 0.0e+00 1.4e+01  1100  0  0  0   1100  0  0 17  2332
PCSetUp               56 1.0 1.1368e-03 1.1 1.12e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2727
PCSetUpOnBlocks       28 1.0 1.8835e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  6  0  0  0   0  6  0  0  0  1176
PCApply               28 1.0 1.0736e-03 1.2 1.16e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 93  0  0  0   1 93  0  0  0  2991
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.19209e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.611e-01      1.00002   1.611e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.330e+06      1.22124   1.229e+06  3.688e+06
Flops/sec:            8.256e+06      1.22127   7.632e+06  2.290e+07
MPI Messages:         1.080e+02      1.71429   8.050e+01  2.415e+02
MPI Message Lengths:  9.104e+04      2.24779   7.543e+02  1.822e+05
MPI Reductions:       2.845e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.0601e-01  65.8%  3.6883e+06 100.0%  1.995e+02  82.6%  7.543e+02      100.0%  8.700e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              30 1.0 7.7009e-05 1.2 1.28e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4623
MatLUFactorNum        30 1.0 1.0376e-03 1.2 1.20e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3201
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      30 1.0 3.2902e-04 1.2 0.00e+00 0.0 1.8e+02 1.0e+03 3.0e+01  0  0 75100  0   0  0 90100 34     0
MatAssemblyEnd        30 1.0 7.4053e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 3.7e+01  0  0  5  0  0   1  0  6  0 43     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        30 1.0 1.0777e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               30 1.0 1.8811e-04 2.2 3.87e+03 1.2 0.0e+00 0.0e+00 1.5e+01  0  0  0  0  0   0  0  0  0 17    59
VecCopy               30 1.0 8.1062e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                62 1.0 1.1206e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          30 1.0 1.9622e-04 2.2 3.87e+03 1.2 0.0e+00 0.0e+00 1.5e+01  0  0  0  0  0   0  0  0  0 17    56
KSPSetUp              32 1.0 2.0981e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              30 1.0 1.5621e-03 1.0 1.33e+06 1.2 0.0e+00 0.0e+00 1.5e+01  1100  0  0  0   1100  0  0 17  2361
PCSetUp               60 1.0 1.2047e-03 1.1 1.20e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2757
PCSetUpOnBlocks       30 1.0 1.8835e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  6  0  0  0   0  6  0  0  0  1176
PCApply               30 1.0 1.1506e-03 1.2 1.25e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 94  0  0  0   1 94  0  0  0  3003
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.685e-01      1.00001   1.685e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.419e+06      1.22124   1.311e+06  3.934e+06
Flops/sec:            8.421e+06      1.22126   7.784e+06  2.335e+07
MPI Messages:         1.150e+02      1.71642   8.567e+01  2.570e+02
MPI Message Lengths:  9.710e+04      2.24790   7.560e+02  1.943e+05
MPI Reductions:       3.048e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.0960e-01  65.1%  3.9342e+06 100.0%  2.120e+02  82.5%  7.560e+02      100.0%  9.200e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              32 1.0 8.2016e-05 1.2 1.36e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4630
MatLUFactorNum        32 1.0 1.1036e-03 1.2 1.28e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3210
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      32 1.0 3.4690e-04 1.2 0.00e+00 0.0 1.9e+02 1.0e+03 3.2e+01  0  0 75100  0   0  0 91100 35     0
MatAssemblyEnd        32 1.0 7.7653e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 3.9e+01  0  0  5  0  0   1  0  6  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        32 1.0 1.1063e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               32 1.0 2.0003e-04 2.3 4.13e+03 1.2 0.0e+00 0.0e+00 1.6e+01  0  0  0  0  0   0  0  0  0 17    59
VecCopy               32 1.0 8.1062e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                66 1.0 1.1206e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          32 1.0 2.0909e-04 2.3 4.13e+03 1.2 0.0e+00 0.0e+00 1.6e+01  0  0  0  0  0   0  0  0  0 17    56
KSPSetUp              34 1.0 2.0981e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              32 1.0 1.6491e-03 1.0 1.42e+06 1.2 0.0e+00 0.0e+00 1.6e+01  1100  0  0  0   1100  0  0 17  2386
PCSetUp               64 1.0 1.2729e-03 1.1 1.28e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2783
PCSetUpOnBlocks       32 1.0 1.8835e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  6  0  0  0   0  6  0  0  0  1176
PCApply               32 1.0 1.2295e-03 1.2 1.33e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 94  0  0  0   1 94  0  0  0  3010
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.758e-01      1.00002   1.758e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.507e+06      1.22124   1.393e+06  4.180e+06
Flops/sec:            8.573e+06      1.22127   7.924e+06  2.377e+07
MPI Messages:         1.220e+02      1.71831   9.083e+01  2.725e+02
MPI Message Lengths:  1.032e+05      2.24801   7.574e+02  2.064e+05
MPI Reductions:       3.251e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.1321e-01  64.4%  4.1800e+06 100.0%  2.245e+02  82.4%  7.574e+02      100.0%  9.700e+01   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              34 1.0 8.6069e-05 1.2 1.45e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4688
MatLUFactorNum        34 1.0 1.1697e-03 1.2 1.36e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3218
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      34 1.0 3.6478e-04 1.2 0.00e+00 0.0 2.0e+02 1.0e+03 3.4e+01  0  0 75100  0   0  0 91100 35     0
MatAssemblyEnd        34 1.0 8.1253e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 4.1e+01  0  0  4  0  0   1  0  5  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        34 1.0 1.1373e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               34 1.0 2.1195e-04 2.3 4.39e+03 1.2 0.0e+00 0.0e+00 1.7e+01  0  0  0  0  0   0  0  0  0 18    59
VecCopy               34 1.0 8.1062e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                70 1.0 1.1206e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          34 1.0 2.2197e-04 2.4 4.39e+03 1.2 0.0e+00 0.0e+00 1.7e+01  0  0  0  0  0   0  0  0  0 18    57
KSPSetUp              36 1.0 2.1935e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              34 1.0 1.7362e-03 1.0 1.51e+06 1.2 0.0e+00 0.0e+00 1.7e+01  1100  0  0  0   2100  0  0 18  2408
PCSetUp               68 1.0 1.3411e-03 1.1 1.36e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2807
PCSetUpOnBlocks       34 1.0 1.8835e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  1176
PCApply               34 1.0 1.3075e-03 1.2 1.42e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 94  0  0  0   1 94  0  0  0  3018
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.833e-01      1.00001   1.832e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.596e+06      1.22124   1.475e+06  4.426e+06
Flops/sec:            8.709e+06      1.22126   8.051e+06  2.415e+07
MPI Messages:         1.290e+02      1.72000   9.600e+01  2.880e+02
MPI Message Lengths:  1.092e+05      2.24810   7.587e+02  2.185e+05
MPI Reductions:       3.454e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.1686e-01  63.8%  4.4259e+06 100.0%  2.370e+02  82.3%  7.587e+02      100.0%  1.020e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              36 1.0 9.1076e-05 1.2 1.54e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4691
MatLUFactorNum        36 1.0 1.2379e-03 1.2 1.44e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3220
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      36 1.0 3.8385e-04 1.2 0.00e+00 0.0 2.2e+02 1.0e+03 3.6e+01  0  0 75100  0   0  0 91100 35     0
MatAssemblyEnd        36 1.0 8.4758e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 4.3e+01  0  0  4  0  0   1  0  5  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        36 1.0 1.1683e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               36 1.0 2.2197e-04 2.3 4.64e+03 1.2 0.0e+00 0.0e+00 1.8e+01  0  0  0  0  0   0  0  0  0 18    60
VecCopy               36 1.0 9.0599e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                74 1.0 1.3113e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          36 1.0 2.3198e-04 2.4 4.64e+03 1.2 0.0e+00 0.0e+00 1.8e+01  0  0  0  0  0   0  0  0  0 18    57
KSPSetUp              38 1.0 2.1935e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              36 1.0 1.8263e-03 1.0 1.60e+06 1.2 0.0e+00 0.0e+00 1.8e+01  1100  0  0  0   2100  0  0 18  2423
PCSetUp               72 1.0 1.4122e-03 1.1 1.44e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2822
PCSetUpOnBlocks       36 1.0 1.8835e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  1176
PCApply               36 1.0 1.3885e-03 1.2 1.51e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 95  0  0  0   1 95  0  0  0  3018
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.906e-01      1.00001   1.906e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.685e+06      1.22124   1.557e+06  4.672e+06
Flops/sec:            8.838e+06      1.22126   8.169e+06  2.451e+07
MPI Messages:         1.360e+02      1.72152   1.012e+02  3.035e+02
MPI Message Lengths:  1.153e+05      2.24818   7.599e+02  2.306e+05
MPI Reductions:       3.658e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.2046e-01  63.2%  4.6718e+06 100.0%  2.495e+02  82.2%  7.599e+02      100.0%  1.070e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              38 1.0 9.6083e-05 1.2 1.62e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4693
MatLUFactorNum        38 1.0 1.3039e-03 1.2 1.52e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3226
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      38 1.0 4.0293e-04 1.2 0.00e+00 0.0 2.3e+02 1.0e+03 3.8e+01  0  0 75100  0   0  0 91100 36     0
MatAssemblyEnd        38 1.0 8.8263e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 4.5e+01  0  0  4  0  0   1  0  5  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        38 1.0 1.1873e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               38 1.0 2.3389e-04 2.3 4.90e+03 1.2 0.0e+00 0.0e+00 1.9e+01  0  0  0  0  0   0  0  0  0 18    60
VecCopy               38 1.0 1.0014e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                78 1.0 1.3351e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          38 1.0 2.4390e-04 2.4 4.90e+03 1.2 0.0e+00 0.0e+00 1.9e+01  0  0  0  0  0   0  0  0  0 18    57
KSPSetUp              40 1.0 2.1935e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              38 1.0 1.9133e-03 1.0 1.68e+06 1.2 0.0e+00 0.0e+00 1.9e+01  1100  0  0  0   2100  0  0 18  2442
PCSetUp               76 1.0 1.4801e-03 1.1 1.52e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2842
PCSetUpOnBlocks       38 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  1170
PCApply               38 1.0 1.4665e-03 1.2 1.60e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 95  0  0  0   1 95  0  0  0  3025
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.980e-01      1.00001   1.980e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.773e+06      1.22124   1.639e+06  4.918e+06
Flops/sec:            8.956e+06      1.22126   8.279e+06  2.484e+07
MPI Messages:         1.430e+02      1.72289   1.063e+02  3.190e+02
MPI Message Lengths:  1.213e+05      2.24825   7.609e+02  2.427e+05
MPI Reductions:       3.861e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.2406e-01  62.7%  4.9177e+06 100.0%  2.620e+02  82.1%  7.609e+02      100.0%  1.120e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              40 1.0 1.0109e-04 1.2 1.71e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4695
MatLUFactorNum        40 1.0 1.3697e-03 1.2 1.60e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3233
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      40 1.0 4.2105e-04 1.2 0.00e+00 0.0 2.4e+02 1.0e+03 4.0e+01  0  0 75100  0   0  0 92100 36     0
MatAssemblyEnd        40 1.0 9.1743e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 4.7e+01  0  0  4  0  0   1  0  5  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        40 1.0 1.2159e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               40 1.0 2.4676e-04 2.4 5.16e+03 1.2 0.0e+00 0.0e+00 2.0e+01  0  0  0  0  0   0  0  0  0 18    60
VecCopy               40 1.0 1.0014e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                82 1.0 1.4067e-05 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          40 1.0 2.5678e-04 2.4 5.16e+03 1.2 0.0e+00 0.0e+00 2.0e+01  0  0  0  0  0   0  0  0  0 18    57
KSPSetUp              42 1.0 2.1935e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              40 1.0 2.0003e-03 1.0 1.77e+06 1.2 0.0e+00 0.0e+00 2.0e+01  1100  0  0  0   2100  0  0 18  2458
PCSetUp               80 1.0 1.5469e-03 1.1 1.60e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2863
PCSetUpOnBlocks       40 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  1170
PCApply               40 1.0 1.5445e-03 1.2 1.69e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 95  0  0  0   1 95  0  0  0  3031
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.055e-01      1.00002   2.055e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.862e+06      1.22124   1.721e+06  5.164e+06
Flops/sec:            9.062e+06      1.22126   8.377e+06  2.513e+07
MPI Messages:         1.500e+02      1.72414   1.115e+02  3.345e+02
MPI Message Lengths:  1.274e+05      2.24831   7.619e+02  2.548e+05
MPI Reductions:       4.064e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.2775e-01  62.2%  5.1636e+06 100.0%  2.745e+02  82.1%  7.619e+02      100.0%  1.170e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              42 1.0 1.0610e-04 1.2 1.79e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4698
MatLUFactorNum        42 1.0 1.4348e-03 1.2 1.68e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3241
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      42 1.0 4.4489e-04 1.2 0.00e+00 0.0 2.5e+02 1.0e+03 4.2e+01  0  0 75100  0   0  0 92100 36     0
MatAssemblyEnd        42 1.0 9.5463e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 4.9e+01  0  0  4  0  0   1  0  4  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        42 1.0 1.2469e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               42 1.0 2.5868e-04 2.5 5.42e+03 1.2 0.0e+00 0.0e+00 2.1e+01  0  0  0  0  0   0  0  0  0 18    60
VecCopy               42 1.0 1.0967e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                86 1.0 1.5020e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          42 1.0 2.6870e-04 2.4 5.42e+03 1.2 0.0e+00 0.0e+00 2.1e+01  0  0  0  0  0   0  0  0  0 18    58
KSPSetUp              44 1.0 2.1935e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              42 1.0 2.0862e-03 1.0 1.86e+06 1.2 0.0e+00 0.0e+00 2.1e+01  1100  0  0  0   2100  0  0 18  2475
PCSetUp               84 1.0 1.6150e-03 1.1 1.68e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2879
PCSetUpOnBlocks       42 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1170
PCApply               42 1.0 1.6224e-03 1.2 1.78e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 95  0  0  0   1 95  0  0  0  3037
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.130e-01      1.00001   2.130e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                1.951e+06      1.22124   1.803e+06  5.409e+06
Flops/sec:            9.160e+06      1.22126   8.467e+06  2.540e+07
MPI Messages:         1.570e+02      1.72527   1.167e+02  3.500e+02
MPI Message Lengths:  1.334e+05      2.24837   7.628e+02  2.670e+05
MPI Reductions:       4.267e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.3147e-01  61.7%  5.4095e+06 100.0%  2.870e+02  82.0%  7.628e+02      100.0%  1.220e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              44 1.0 1.1110e-04 1.2 1.88e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4699
MatLUFactorNum        44 1.0 1.4997e-03 1.2 1.76e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3248
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      44 1.0 4.6492e-04 1.2 0.00e+00 0.0 2.6e+02 1.0e+03 4.4e+01  0  0 75100  0   0  0 92100 36     0
MatAssemblyEnd        44 1.0 9.8872e-04 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 5.1e+01  0  0  3  0  0   1  0  4  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        44 1.0 1.2755e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               44 1.0 2.6655e-04 2.4 5.68e+03 1.2 0.0e+00 0.0e+00 2.2e+01  0  0  0  0  0   0  0  0  0 18    61
VecCopy               44 1.0 1.0967e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                90 1.0 1.5020e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          44 1.0 2.7657e-04 2.4 5.68e+03 1.2 0.0e+00 0.0e+00 2.2e+01  0  0  0  0  0   0  0  0  0 18    59
KSPSetUp              46 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              44 1.0 2.1720e-03 1.0 1.95e+06 1.2 0.0e+00 0.0e+00 2.2e+01  1100  0  0  0   2100  0  0 18  2491
PCSetUp               88 1.0 1.6820e-03 1.1 1.76e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2896
PCSetUpOnBlocks       44 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1170
PCApply               44 1.0 1.7006e-03 1.2 1.87e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 96  0  0  0   1 96  0  0  0  3041
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.62125e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.203e-01      1.00001   2.203e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.039e+06      1.22124   1.885e+06  5.655e+06
Flops/sec:            9.256e+06      1.22126   8.556e+06  2.567e+07
MPI Messages:         1.640e+02      1.72632   1.218e+02  3.655e+02
MPI Message Lengths:  1.395e+05      2.24843   7.636e+02  2.791e+05
MPI Reductions:       4.470e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.3507e-01  61.3%  5.6554e+06 100.0%  2.995e+02  81.9%  7.636e+02      100.0%  1.270e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              46 1.0 1.1611e-04 1.2 1.96e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4701
MatLUFactorNum        46 1.0 1.5657e-03 1.2 1.84e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3253
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      46 1.0 4.8399e-04 1.2 0.00e+00 0.0 2.8e+02 1.0e+03 4.6e+01  0  0 76100  0   0  0 92100 36     0
MatAssemblyEnd        46 1.0 1.0247e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 5.3e+01  0  0  3  0  0   1  0  4  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        46 1.0 1.2946e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               46 1.0 2.7657e-04 2.4 5.93e+03 1.2 0.0e+00 0.0e+00 2.3e+01  0  0  0  0  0   0  0  0  0 18    61
VecCopy               46 1.0 1.2159e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                94 1.0 1.6212e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          46 1.0 2.8753e-04 2.4 5.93e+03 1.2 0.0e+00 0.0e+00 2.3e+01  0  0  0  0  0   0  0  0  0 18    59
KSPSetUp              48 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              46 1.0 2.2600e-03 1.0 2.04e+06 1.2 0.0e+00 0.0e+00 2.3e+01  1100  0  0  0   2100  0  0 18  2502
PCSetUp               92 1.0 1.7509e-03 1.1 1.84e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2908
PCSetUpOnBlocks       46 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1170
PCApply               46 1.0 1.7786e-03 1.2 1.95e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 96  0  0  0   1 96  0  0  0  3046
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.277e-01      1.00001   2.277e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.128e+06      1.22124   1.967e+06  5.901e+06
Flops/sec:            9.345e+06      1.22126   8.639e+06  2.592e+07
MPI Messages:         1.710e+02      1.72727   1.270e+02  3.810e+02
MPI Message Lengths:  1.455e+05      2.24848   7.643e+02  2.912e+05
MPI Reductions:       4.673e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.3866e-01  60.9%  5.9012e+06 100.0%  3.120e+02  81.9%  7.643e+02      100.0%  1.320e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              48 1.0 1.2112e-04 1.2 2.05e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4703
MatLUFactorNum        48 1.0 1.6317e-03 1.2 1.92e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3257
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      48 1.0 5.0211e-04 1.2 0.00e+00 0.0 2.9e+02 1.0e+03 4.8e+01  0  0 76100  0   0  0 92100 36     0
MatAssemblyEnd        48 1.0 1.0598e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 5.5e+01  0  0  3  0  0   1  0  4  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        48 1.0 1.3161e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               48 1.0 2.8968e-04 2.5 6.19e+03 1.2 0.0e+00 0.0e+00 2.4e+01  0  0  0  0  0   0  0  0  0 18    61
VecCopy               48 1.0 1.3113e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                98 1.0 1.6212e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          48 1.0 3.0065e-04 2.5 6.19e+03 1.2 0.0e+00 0.0e+00 2.4e+01  0  0  0  0  0   0  0  0  0 18    59
KSPSetUp              50 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              48 1.0 2.3470e-03 1.0 2.13e+06 1.2 0.0e+00 0.0e+00 2.4e+01  1100  0  0  0   2100  0  0 18  2514
PCSetUp               96 1.0 1.8201e-03 1.1 1.92e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2920
PCSetUpOnBlocks       48 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1170
PCApply               48 1.0 1.8566e-03 1.2 2.04e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 96  0  0  0   1 96  0  0  0  3050
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.19209e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.351e-01      1.00001   2.351e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.217e+06      1.22124   2.049e+06  6.147e+06
Flops/sec:            9.430e+06      1.22126   8.717e+06  2.615e+07
MPI Messages:         1.780e+02      1.72816   1.322e+02  3.965e+02
MPI Message Lengths:  1.516e+05      2.24852   7.650e+02  3.033e+05
MPI Reductions:       4.876e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.4227e-01  60.5%  6.1471e+06 100.0%  3.245e+02  81.8%  7.650e+02      100.0%  1.370e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              50 1.0 1.2612e-04 1.2 2.13e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4704
MatLUFactorNum        50 1.0 1.6978e-03 1.2 2.00e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3260
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      50 1.0 5.2118e-04 1.2 0.00e+00 0.0 3.0e+02 1.0e+03 5.0e+01  0  0 76100  0   0  0 92100 36     0
MatAssemblyEnd        50 1.0 1.0939e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 5.7e+01  0  0  3  0  0   1  0  4  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        50 1.0 1.3351e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               50 1.0 3.0184e-04 2.5 6.45e+03 1.2 0.0e+00 0.0e+00 2.5e+01  0  0  0  0  0   0  0  0  0 18    61
VecCopy               50 1.0 1.3113e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               102 1.0 1.6212e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          50 1.0 3.1281e-04 2.5 6.45e+03 1.2 0.0e+00 0.0e+00 2.5e+01  0  0  0  0  0   0  0  0  0 18    59
KSPSetUp              52 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              50 1.0 2.4340e-03 1.0 2.22e+06 1.2 0.0e+00 0.0e+00 2.5e+01  1100  0  0  0   2100  0  0 18  2526
PCSetUp              100 1.0 1.8880e-03 1.1 2.00e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2932
PCSetUpOnBlocks       50 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1170
PCApply               50 1.0 1.9345e-03 1.2 2.13e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 96  0  0  0   1 96  0  0  0  3054
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.427e-01      1.00001   2.427e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.305e+06      1.22124   2.131e+06  6.393e+06
Flops/sec:            9.500e+06      1.22126   8.782e+06  2.635e+07
MPI Messages:         1.850e+02      1.72897   1.373e+02  4.120e+02
MPI Message Lengths:  1.577e+05      2.24857   7.656e+02  3.154e+05
MPI Reductions:       5.079e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.4613e-01  60.2%  6.3930e+06 100.0%  3.370e+02  81.8%  7.656e+02      100.0%  1.420e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              52 1.0 1.2994e-04 1.2 2.22e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4749
MatLUFactorNum        52 1.0 1.7676e-03 1.2 2.08e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3257
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      52 1.0 5.4336e-04 1.2 0.00e+00 0.0 3.1e+02 1.0e+03 5.2e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        52 1.0 1.1280e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 5.9e+01  0  0  3  0  0   1  0  4  0 42     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        52 1.0 1.3542e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               52 1.0 3.0875e-04 2.5 6.71e+03 1.2 0.0e+00 0.0e+00 2.6e+01  0  0  0  0  0   0  0  0  0 18    62
VecCopy               52 1.0 1.3113e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               106 1.0 1.7166e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          52 1.0 3.1972e-04 2.5 6.71e+03 1.2 0.0e+00 0.0e+00 2.6e+01  0  0  0  0  0   0  0  0  0 18    60
KSPSetUp              54 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              52 1.0 2.5249e-03 1.0 2.31e+06 1.2 0.0e+00 0.0e+00 2.6e+01  1100  0  0  0   2100  0  0 18  2532
PCSetUp              104 1.0 1.9598e-03 1.1 2.08e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2937
PCSetUpOnBlocks       52 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1170
PCApply               52 1.0 2.0156e-03 1.2 2.22e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 96  0  0  0   1 96  0  0  0  3052
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.501e-01      1.00001   2.501e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.394e+06      1.22124   2.213e+06  6.639e+06
Flops/sec:            9.574e+06      1.22125   8.850e+06  2.655e+07
MPI Messages:         1.920e+02      1.72973   1.425e+02  4.275e+02
MPI Message Lengths:  1.637e+05      2.24861   7.662e+02  3.275e+05
MPI Reductions:       5.282e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.4974e-01  59.9%  6.6389e+06 100.0%  3.495e+02  81.8%  7.662e+02      100.0%  1.470e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              54 1.0 1.3399e-04 1.2 2.30e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4782
MatLUFactorNum        54 1.0 1.8337e-03 1.2 2.16e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3260
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      54 1.0 5.6338e-04 1.2 0.00e+00 0.0 3.2e+02 1.0e+03 5.4e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        54 1.0 1.1630e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 6.1e+01  0  0  3  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        54 1.0 1.3852e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               54 1.0 3.1877e-04 2.5 6.97e+03 1.2 0.0e+00 0.0e+00 2.7e+01  0  0  0  0  0   0  0  0  0 18    63
VecCopy               54 1.0 1.3113e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               110 1.0 1.9312e-05 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          54 1.0 3.2973e-04 2.5 6.97e+03 1.2 0.0e+00 0.0e+00 2.7e+01  0  0  0  0  0   0  0  0  0 18    60
KSPSetUp              56 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              54 1.0 2.6119e-03 1.0 2.39e+06 1.2 0.0e+00 0.0e+00 2.7e+01  1100  0  0  0   2100  0  0 18  2542
PCSetUp              108 1.0 2.0280e-03 1.1 2.16e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2948
PCSetUpOnBlocks       54 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1170
PCApply               54 1.0 2.0936e-03 1.2 2.31e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 96  0  0  0   1 96  0  0  0  3056
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.574e-01      1.00001   2.574e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.483e+06      1.22124   2.295e+06  6.885e+06
Flops/sec:            9.644e+06      1.22125   8.915e+06  2.674e+07
MPI Messages:         1.990e+02      1.73043   1.477e+02  4.430e+02
MPI Message Lengths:  1.698e+05      2.24864   7.667e+02  3.396e+05
MPI Reductions:       5.486e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.5334e-01  59.6%  6.8848e+06 100.0%  3.620e+02  81.7%  7.667e+02      100.0%  1.520e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              56 1.0 1.3900e-04 1.2 2.39e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4781
MatLUFactorNum        56 1.0 1.8997e-03 1.2 2.24e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3263
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      56 1.0 5.8126e-04 1.2 0.00e+00 0.0 3.4e+02 1.0e+03 5.6e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        56 1.0 1.1969e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 6.3e+01  0  0  3  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        56 1.0 1.4043e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               56 1.0 3.3188e-04 2.5 7.22e+03 1.2 0.0e+00 0.0e+00 2.8e+01  0  0  0  0  0   0  0  0  0 18    62
VecCopy               56 1.0 1.3113e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               114 1.0 1.9312e-05 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          56 1.0 3.4285e-04 2.5 7.22e+03 1.2 0.0e+00 0.0e+00 2.8e+01  0  0  0  0  0   0  0  0  0 18    60
KSPSetUp              58 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              56 1.0 2.6989e-03 1.0 2.48e+06 1.2 0.0e+00 0.0e+00 2.8e+01  1100  0  0  0   2100  0  0 18  2551
PCSetUp              112 1.0 2.0969e-03 1.1 2.24e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2957
PCSetUpOnBlocks       56 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1170
PCApply               56 1.0 2.1706e-03 1.2 2.40e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 96  0  0  0   1 96  0  0  0  3060
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.81198e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.648e-01      1.00001   2.648e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.571e+06      1.22124   2.377e+06  7.131e+06
Flops/sec:            9.710e+06      1.22125   8.976e+06  2.693e+07
MPI Messages:         2.060e+02      1.73109   1.528e+02  4.585e+02
MPI Message Lengths:  1.758e+05      2.24868   7.672e+02  3.518e+05
MPI Reductions:       5.689e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.5694e-01  59.3%  7.1307e+06 100.0%  3.745e+02  81.7%  7.672e+02      100.0%  1.570e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              58 1.0 1.4400e-04 1.2 2.47e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4779
MatLUFactorNum        58 1.0 1.9646e-03 1.2 2.32e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3268
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      58 1.0 5.9915e-04 1.2 0.00e+00 0.0 3.5e+02 1.0e+03 5.8e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        58 1.0 1.2319e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 6.5e+01  0  0  3  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        58 1.0 1.4353e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               58 1.0 3.4380e-04 2.6 7.48e+03 1.2 0.0e+00 0.0e+00 2.9e+01  0  0  0  0  0   0  0  0  0 18    62
VecCopy               58 1.0 1.3113e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               118 1.0 2.0266e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          58 1.0 3.5572e-04 2.5 7.48e+03 1.2 0.0e+00 0.0e+00 2.9e+01  0  0  0  0  0   0  0  0  0 18    60
KSPSetUp              60 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              58 1.0 2.7850e-03 1.0 2.57e+06 1.2 0.0e+00 0.0e+00 2.9e+01  1100  0  0  0   2100  0  0 18  2560
PCSetUp              116 1.0 2.1636e-03 1.1 2.32e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2968
PCSetUpOnBlocks       58 1.0 1.8930e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1170
PCApply               58 1.0 2.2485e-03 1.2 2.48e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3063
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.62125e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.722e-01      1.00001   2.722e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.660e+06      1.22124   2.459e+06  7.377e+06
Flops/sec:            9.773e+06      1.22125   9.034e+06  2.710e+07
MPI Messages:         2.130e+02      1.73171   1.580e+02  4.740e+02
MPI Message Lengths:  1.819e+05      2.24871   7.677e+02  3.639e+05
MPI Reductions:       5.892e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.6052e-01  59.0%  7.3766e+06 100.0%  3.870e+02  81.6%  7.677e+02      100.0%  1.620e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              60 1.0 1.4806e-04 1.2 2.56e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4809
MatLUFactorNum        60 1.0 2.0287e-03 1.2 2.40e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3274
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      60 1.0 6.1703e-04 1.2 0.00e+00 0.0 3.6e+02 1.0e+03 6.0e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        60 1.0 1.2660e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 6.7e+01  0  0  3  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        60 1.0 1.4544e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               60 1.0 3.5477e-04 2.6 7.74e+03 1.2 0.0e+00 0.0e+00 3.0e+01  0  0  0  0  0   0  0  0  0 19    62
VecCopy               60 1.0 1.4067e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               122 1.0 2.0266e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          60 1.0 3.6669e-04 2.6 7.74e+03 1.2 0.0e+00 0.0e+00 3.0e+01  0  0  0  0  0   0  0  0  0 19    60
KSPSetUp              62 1.0 2.2888e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              60 1.0 2.8698e-03 1.0 2.66e+06 1.2 0.0e+00 0.0e+00 3.0e+01  1100  0  0  0   2100  0  0 19  2570
PCSetUp              120 1.0 2.2318e-03 1.1 2.40e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2976
PCSetUpOnBlocks       60 1.0 1.9026e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1164
PCApply               60 1.0 2.3246e-03 1.2 2.57e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3069
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.796e-01      1.00001   2.796e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.749e+06      1.22124   2.541e+06  7.622e+06
Flops/sec:            9.829e+06      1.22125   9.086e+06  2.726e+07
MPI Messages:         2.200e+02      1.73228   1.632e+02  4.895e+02
MPI Message Lengths:  1.879e+05      2.24874   7.681e+02  3.760e+05
MPI Reductions:       6.095e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.6421e-01  58.7%  7.6224e+06 100.0%  3.995e+02  81.6%  7.681e+02      100.0%  1.670e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              62 1.0 1.5306e-04 1.2 2.64e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4807
MatLUFactorNum        62 1.0 2.0947e-03 1.2 2.48e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3277
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      62 1.0 6.3896e-04 1.2 0.00e+00 0.0 3.7e+02 1.0e+03 6.2e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        62 1.0 1.3030e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 6.9e+01  0  0  2  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        62 1.0 1.4830e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               62 1.0 3.6359e-04 2.6 8.00e+03 1.2 0.0e+00 0.0e+00 3.1e+01  0  0  0  0  0   0  0  0  0 19    63
VecCopy               62 1.0 1.5020e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               126 1.0 2.0266e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          62 1.0 3.7551e-04 2.6 8.00e+03 1.2 0.0e+00 0.0e+00 3.1e+01  0  0  0  0  0   0  0  0  0 19    61
KSPSetUp              64 1.0 2.2888e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              62 1.0 2.9581e-03 1.0 2.75e+06 1.2 0.0e+00 0.0e+00 3.1e+01  1100  0  0  0   2100  0  0 19  2577
PCSetUp              124 1.0 2.3017e-03 1.1 2.48e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2982
PCSetUpOnBlocks       62 1.0 1.9026e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1164
PCApply               62 1.0 2.4028e-03 1.2 2.66e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3071
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.870e-01      1.00001   2.870e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.837e+06      1.22124   2.623e+06  7.868e+06
Flops/sec:            9.885e+06      1.22126   9.137e+06  2.741e+07
MPI Messages:         2.270e+02      1.73282   1.683e+02  5.050e+02
MPI Message Lengths:  1.940e+05      2.24877   7.685e+02  3.881e+05
MPI Reductions:       6.298e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.6781e-01  58.5%  7.8683e+06 100.0%  4.120e+02  81.6%  7.685e+02      100.0%  1.720e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              64 1.0 1.5783e-04 1.2 2.73e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4812
MatLUFactorNum        64 1.0 2.1598e-03 1.2 2.56e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3280
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      64 1.0 6.5804e-04 1.2 0.00e+00 0.0 3.8e+02 1.0e+03 6.4e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        64 1.0 1.3402e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 7.1e+01  0  0  2  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        64 1.0 1.5140e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               64 1.0 3.7551e-04 2.6 8.26e+03 1.2 0.0e+00 0.0e+00 3.2e+01  0  0  0  0  0   0  0  0  0 19    63
VecCopy               64 1.0 1.5020e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               130 1.0 2.0266e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          64 1.0 3.8743e-04 2.6 8.26e+03 1.2 0.0e+00 0.0e+00 3.2e+01  0  0  0  0  0   0  0  0  0 19    61
KSPSetUp              66 1.0 2.2888e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              64 1.0 3.0451e-03 1.0 2.84e+06 1.2 0.0e+00 0.0e+00 3.2e+01  1100  0  0  0   2100  0  0 19  2584
PCSetUp              128 1.0 2.3696e-03 1.1 2.56e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2990
PCSetUpOnBlocks       64 1.0 1.9026e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1164
PCApply               64 1.0 2.4807e-03 1.2 2.75e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3073
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.62125e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.944e-01      1.00001   2.944e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                2.926e+06      1.22124   2.705e+06  8.114e+06
Flops/sec:            9.940e+06      1.22125   9.188e+06  2.757e+07
MPI Messages:         2.340e+02      1.73333   1.735e+02  5.205e+02
MPI Message Lengths:  2.000e+05      2.24879   7.689e+02  4.002e+05
MPI Reductions:       6.501e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.7141e-01  58.2%  8.1142e+06 100.0%  4.245e+02  81.6%  7.689e+02      100.0%  1.770e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              66 1.0 1.6284e-04 1.2 2.81e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4810
MatLUFactorNum        66 1.0 2.2249e-03 1.2 2.64e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3284
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      66 1.0 6.7592e-04 1.2 0.00e+00 0.0 4.0e+02 1.0e+03 6.6e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        66 1.0 1.3750e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 7.3e+01  0  0  2  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        66 1.0 1.5330e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               66 1.0 3.8958e-04 2.7 8.51e+03 1.2 0.0e+00 0.0e+00 3.3e+01  0  0  0  0  0   0  0  0  0 19    63
VecCopy               66 1.0 1.6212e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               134 1.0 2.0266e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          66 1.0 4.0245e-04 2.6 8.51e+03 1.2 0.0e+00 0.0e+00 3.3e+01  0  0  0  0  0   0  0  0  0 19    61
KSPSetUp              68 1.0 2.2888e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              66 1.0 3.1321e-03 1.0 2.93e+06 1.2 0.0e+00 0.0e+00 3.3e+01  1100  0  0  0   2100  0  0 19  2591
PCSetUp              132 1.0 2.4385e-03 1.2 2.64e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2996
PCSetUpOnBlocks       66 1.0 1.9026e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1164
PCApply               66 1.0 2.5587e-03 1.2 2.84e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3075
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.017e-01      1.00001   3.017e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.015e+06      1.22124   2.787e+06  8.360e+06
Flops/sec:            9.991e+06      1.22125   9.235e+06  2.771e+07
MPI Messages:         2.410e+02      1.73381   1.787e+02  5.360e+02
MPI Message Lengths:  2.061e+05      2.24882   7.693e+02  4.123e+05
MPI Reductions:       6.704e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.7500e-01  58.0%  8.3601e+06 100.0%  4.370e+02  81.5%  7.693e+02      100.0%  1.820e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              68 1.0 1.6785e-04 1.2 2.90e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4807
MatLUFactorNum        68 1.0 2.2900e-03 1.2 2.72e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3287
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      68 1.0 6.9380e-04 1.2 0.00e+00 0.0 4.1e+02 1.0e+03 6.8e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        68 1.0 1.4110e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 7.5e+01  0  0  2  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        68 1.0 1.5521e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               68 1.0 4.0174e-04 2.7 8.77e+03 1.2 0.0e+00 0.0e+00 3.4e+01  0  0  0  0  0   0  0  0  0 19    62
VecCopy               68 1.0 1.6212e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               138 1.0 2.2411e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          68 1.0 4.1461e-04 2.7 8.77e+03 1.2 0.0e+00 0.0e+00 3.4e+01  0  0  0  0  0   0  0  0  0 19    61
KSPSetUp              70 1.0 2.3842e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              68 1.0 3.2182e-03 1.0 3.01e+06 1.2 0.0e+00 0.0e+00 3.4e+01  1100  0  0  0   2100  0  0 19  2598
PCSetUp              136 1.0 2.5055e-03 1.2 2.72e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3005
PCSetUpOnBlocks       68 1.0 1.9026e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1164
PCApply               68 1.0 2.6367e-03 1.2 2.93e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3077
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.091e-01      1.00001   3.091e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.103e+06      1.22124   2.869e+06  8.606e+06
Flops/sec:            1.004e+07      1.22125   9.279e+06  2.784e+07
MPI Messages:         2.480e+02      1.73427   1.838e+02  5.515e+02
MPI Message Lengths:  2.122e+05      2.24884   7.696e+02  4.244e+05
MPI Reductions:       6.907e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.7860e-01  57.8%  8.6060e+06 100.0%  4.495e+02  81.5%  7.696e+02      100.0%  1.870e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              70 1.0 1.7285e-04 1.2 2.99e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4806
MatLUFactorNum        70 1.0 2.3561e-03 1.2 2.80e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3289
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      70 1.0 7.1168e-04 1.2 0.00e+00 0.0 4.2e+02 1.0e+03 7.0e+01  0  0 76100  0   0  0 93100 37     0
MatAssemblyEnd        70 1.0 1.4458e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 7.7e+01  0  0  2  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        70 1.0 1.5736e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               70 1.0 4.1080e-04 2.7 9.03e+03 1.2 0.0e+00 0.0e+00 3.5e+01  0  0  0  0  0   0  0  0  0 19    63
VecCopy               70 1.0 1.6212e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               142 1.0 2.2411e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          70 1.0 4.2367e-04 2.7 9.03e+03 1.2 0.0e+00 0.0e+00 3.5e+01  0  0  0  0  0   0  0  0  0 19    61
KSPSetUp              72 1.0 2.3842e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              70 1.0 3.3052e-03 1.0 3.10e+06 1.2 0.0e+00 0.0e+00 3.5e+01  1100  0  0  0   2100  0  0 19  2604
PCSetUp              140 1.0 2.5735e-03 1.2 2.80e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3011
PCSetUpOnBlocks       70 1.0 1.9145e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1157
PCApply               70 1.0 2.7146e-03 1.2 3.01e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3079
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.166e-01      1.00001   3.166e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.192e+06      1.22124   2.951e+06  8.852e+06
Flops/sec:            1.008e+07      1.22125   9.321e+06  2.796e+07
MPI Messages:         2.550e+02      1.73469   1.890e+02  5.670e+02
MPI Message Lengths:  2.182e+05      2.24886   7.699e+02  4.366e+05
MPI Reductions:       7.110e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.8220e-01  57.6%  8.8519e+06 100.0%  4.620e+02  81.5%  7.699e+02      100.0%  1.920e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              72 1.0 1.7691e-04 1.2 3.07e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4830
MatLUFactorNum        72 1.0 2.4221e-03 1.2 2.88e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3291
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      72 1.0 7.3075e-04 1.2 0.00e+00 0.0 4.3e+02 1.0e+03 7.2e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        72 1.0 1.4808e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 7.9e+01  0  0  2  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        72 1.0 1.6046e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               72 1.0 4.2462e-04 2.7 9.29e+03 1.2 0.0e+00 0.0e+00 3.6e+01  0  0  0  0  0   0  0  0  0 19    63
VecCopy               72 1.0 1.6212e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               146 1.0 2.3365e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          72 1.0 4.3750e-04 2.7 9.29e+03 1.2 0.0e+00 0.0e+00 3.6e+01  0  0  0  0  0   0  0  0  0 19    61
KSPSetUp              74 1.0 2.3842e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              72 1.0 3.3922e-03 1.0 3.19e+06 1.2 0.0e+00 0.0e+00 3.6e+01  1100  0  0  0   2100  0  0 19  2609
PCSetUp              144 1.0 2.6414e-03 1.2 2.88e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3018
PCSetUpOnBlocks       72 1.0 1.9145e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1157
PCApply               72 1.0 2.7916e-03 1.2 3.10e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3082
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.239e-01      1.00001   3.239e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.281e+06      1.22124   3.033e+06  9.098e+06
Flops/sec:            1.013e+07      1.22125   9.362e+06  2.809e+07
MPI Messages:         2.620e+02      1.73510   1.942e+02  5.825e+02
MPI Message Lengths:  2.243e+05      2.24888   7.703e+02  4.487e+05
MPI Reductions:       7.313e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.8580e-01  57.4%  9.0977e+06 100.0%  4.745e+02  81.5%  7.703e+02      100.0%  1.970e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              74 1.0 1.8096e-04 1.2 3.16e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4853
MatLUFactorNum        74 1.0 2.4879e-03 1.2 2.96e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3293
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      74 1.0 7.4768e-04 1.2 0.00e+00 0.0 4.4e+02 1.0e+03 7.4e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        74 1.0 1.5159e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 8.1e+01  0  0  2  0  0   1  0  3  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        74 1.0 1.6356e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               74 1.0 4.3654e-04 2.8 9.55e+03 1.2 0.0e+00 0.0e+00 3.7e+01  0  0  0  0  0   0  0  0  0 19    63
VecCopy               74 1.0 1.7166e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               150 1.0 2.3365e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          74 1.0 4.5037e-04 2.7 9.55e+03 1.2 0.0e+00 0.0e+00 3.7e+01  0  0  0  0  0   0  0  0  0 19    61
KSPSetUp              76 1.0 2.3842e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              74 1.0 3.4792e-03 1.0 3.28e+06 1.2 0.0e+00 0.0e+00 3.7e+01  1100  0  0  0   2100  0  0 19  2615
PCSetUp              148 1.0 2.7103e-03 1.2 2.96e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3023
PCSetUpOnBlocks       74 1.0 1.9145e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1157
PCApply               74 1.0 2.8687e-03 1.2 3.19e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3085
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.00543e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.362e-01      1.00001   3.362e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.369e+06      1.22124   3.115e+06  9.344e+06
Flops/sec:            1.002e+07      1.22125   9.265e+06  2.779e+07
MPI Messages:         2.690e+02      1.73548   1.993e+02  5.980e+02
MPI Message Lengths:  2.303e+05      2.24890   7.705e+02  4.608e+05
MPI Reductions:       7.516e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.9320e-01  57.5%  9.3436e+06 100.0%  4.870e+02  81.4%  7.705e+02      100.0%  2.020e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              76 1.0 1.8573e-04 1.2 3.24e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4856
MatLUFactorNum        76 1.0 2.5530e-03 1.2 3.04e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3296
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      76 1.0 7.9179e-04 1.2 0.00e+00 0.0 4.6e+02 1.0e+03 7.6e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        76 1.0 1.6057e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 8.3e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        76 1.0 1.7953e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               76 1.0 5.5456e-04 2.0 9.80e+03 1.2 0.0e+00 0.0e+00 3.8e+01  0  0  0  0  0   0  0  0  0 19    51
VecCopy               76 1.0 1.7166e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               154 1.0 2.5272e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          76 1.0 5.6839e-04 2.0 9.80e+03 1.2 0.0e+00 0.0e+00 3.8e+01  0  0  0  0  0   0  0  0  0 19    49
KSPSetUp              78 1.0 2.3842e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              76 1.0 3.6752e-03 1.0 3.37e+06 1.2 0.0e+00 0.0e+00 3.8e+01  1100  0  0  0   2100  0  0 19  2542
PCSetUp              152 1.0 2.7773e-03 1.2 3.04e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3029
PCSetUpOnBlocks       76 1.0 1.9336e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1145
PCApply               76 1.0 2.9466e-03 1.2 3.28e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3086
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.38554e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.511e-01      1.00000   3.511e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.458e+06      1.22124   3.197e+06  9.590e+06
Flops/sec:            9.850e+06      1.22125   9.105e+06  2.732e+07
MPI Messages:         2.760e+02      1.73585   2.045e+02  6.135e+02
MPI Message Lengths:  2.364e+05      2.24892   7.708e+02  4.729e+05
MPI Reductions:       7.720e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.9971e-01  56.9%  9.5895e+06 100.0%  4.995e+02  81.4%  7.708e+02      100.0%  2.070e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              78 1.0 1.9073e-04 1.2 3.33e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4853
MatLUFactorNum        78 1.0 2.6190e-03 1.2 3.12e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3297
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      78 1.0 8.2588e-04 1.2 0.00e+00 0.0 4.7e+02 1.0e+03 7.8e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        78 1.0 1.6859e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 8.5e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        78 1.0 1.8334e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               78 1.0 6.3157e-04 2.2 1.01e+04 1.2 0.0e+00 0.0e+00 3.9e+01  0  0  0  0  0   0  0  0  0 19    46
VecCopy               78 1.0 1.7166e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               158 1.0 2.6464e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          78 1.0 6.4635e-04 2.1 1.01e+04 1.2 0.0e+00 0.0e+00 3.9e+01  0  0  0  0  0   0  0  0  0 19    45
KSPSetUp              80 1.0 2.3842e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              78 1.0 3.8292e-03 1.0 3.46e+06 1.2 0.0e+00 0.0e+00 3.9e+01  1100  0  0  0   2100  0  0 19  2504
PCSetUp              156 1.0 2.8474e-03 1.2 3.12e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3033
PCSetUpOnBlocks       78 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               78 1.0 3.0255e-03 1.2 3.37e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3087
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
Average time for MPI_Barrier(): 3.8147e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.653e-01      1.00001   3.653e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.547e+06      1.22124   3.278e+06  9.835e+06
Flops/sec:            9.710e+06      1.22125   8.976e+06  2.693e+07
MPI Messages:         2.830e+02      1.73620   2.097e+02  6.290e+02
MPI Message Lengths:  2.424e+05      2.24894   7.711e+02  4.850e+05
MPI Reductions:       7.923e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.0578e-01  56.3%  9.8354e+06 100.0%  5.120e+02  81.4%  7.711e+02      100.0%  2.120e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              80 1.0 1.9670e-04 1.2 3.41e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4826
MatLUFactorNum        80 1.0 2.6851e-03 1.2 3.20e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3298
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      80 1.0 8.5688e-04 1.2 0.00e+00 0.0 4.8e+02 1.0e+03 8.0e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        80 1.0 1.7498e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 8.7e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        80 1.0 1.8740e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               80 1.0 6.8164e-04 2.3 1.03e+04 1.2 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  0   0  0  0  0 19    43
VecCopy               80 1.0 1.7643e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               162 1.0 2.6464e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          80 1.0 6.9642e-04 2.3 1.03e+04 1.2 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  0   0  0  0  0 19    42
KSPSetUp              82 1.0 2.3842e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              80 1.0 3.9670e-03 1.0 3.55e+06 1.2 0.0e+00 0.0e+00 4.0e+01  1100  0  0  0   2100  0  0 19  2479
PCSetUp              160 1.0 2.9297e-03 1.2 3.20e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3023
PCSetUpOnBlocks       80 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               80 1.0 3.1047e-03 1.2 3.46e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 97  0  0  0   1 97  0  0  0  3087
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.8147e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.790e-01      1.00000   3.790e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.635e+06      1.22124   3.360e+06  1.008e+07
Flops/sec:            9.593e+06      1.22125   8.867e+06  2.660e+07
MPI Messages:         2.900e+02      1.73653   2.148e+02  6.445e+02
MPI Message Lengths:  2.485e+05      2.24895   7.713e+02  4.971e+05
MPI Reductions:       8.126e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.1162e-01  55.8%  1.0081e+07 100.0%  5.245e+02  81.4%  7.713e+02      100.0%  2.170e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              82 1.0 2.0361e-04 1.2 3.50e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4779
MatLUFactorNum        82 1.0 2.7797e-03 1.2 3.28e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3266
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      82 1.0 8.9383e-04 1.2 0.00e+00 0.0 4.9e+02 1.0e+03 8.2e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        82 1.0 1.8158e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 8.9e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        82 1.0 1.9121e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               82 1.0 7.4482e-04 2.5 1.06e+04 1.2 0.0e+00 0.0e+00 4.1e+01  0  0  0  0  0   0  0  0  0 19    41
VecCopy               82 1.0 1.8358e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               166 1.0 2.6464e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          82 1.0 7.5960e-04 2.4 1.06e+04 1.2 0.0e+00 0.0e+00 4.1e+01  0  0  0  0  0   0  0  0  0 19    40
KSPSetUp              84 1.0 2.3842e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              82 1.0 4.1142e-03 1.0 3.64e+06 1.2 0.0e+00 0.0e+00 4.1e+01  1100  0  0  0   2100  0  0 19  2450
PCSetUp              164 1.0 3.0468e-03 1.2 3.28e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2980
PCSetUpOnBlocks       82 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               82 1.0 3.2277e-03 1.2 3.54e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  3045
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.38554e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.920e-01      1.00000   3.920e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.724e+06      1.22124   3.442e+06  1.033e+07
Flops/sec:            9.500e+06      1.22125   8.781e+06  2.634e+07
MPI Messages:         2.970e+02      1.73684   2.200e+02  6.600e+02
MPI Message Lengths:  2.546e+05      2.24897   7.716e+02  5.092e+05
MPI Reductions:       8.329e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.1697e-01  55.3%  1.0327e+07 100.0%  5.370e+02  81.4%  7.716e+02      100.0%  2.220e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              84 1.0 2.0957e-04 1.2 3.58e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4756
MatLUFactorNum        84 1.0 2.8667e-03 1.2 3.35e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3244
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      84 1.0 9.2196e-04 1.2 0.00e+00 0.0 5.0e+02 1.0e+03 8.4e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        84 1.0 1.8709e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 9.1e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        84 1.0 1.9407e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               84 1.0 7.8678e-04 2.6 1.08e+04 1.2 0.0e+00 0.0e+00 4.2e+01  0  0  0  0  0   0  0  0  0 19    39
VecCopy               84 1.0 1.8597e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               170 1.0 2.8610e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          84 1.0 8.0252e-04 2.5 1.08e+04 1.2 0.0e+00 0.0e+00 4.2e+01  0  0  0  0  0   0  0  0  0 19    39
KSPSetUp              86 1.0 2.4796e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              84 1.0 4.2312e-03 1.0 3.72e+06 1.2 0.0e+00 0.0e+00 4.2e+01  1100  0  0  0   2100  0  0 19  2441
PCSetUp              168 1.0 3.1366e-03 1.2 3.35e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2965
PCSetUpOnBlocks       84 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               84 1.0 3.3298e-03 1.2 3.63e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  3026
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.38554e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.046e-01      1.00000   4.046e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.813e+06      1.22124   3.524e+06  1.057e+07
Flops/sec:            9.424e+06      1.22125   8.711e+06  2.613e+07
MPI Messages:         3.040e+02      1.73714   2.252e+02  6.755e+02
MPI Message Lengths:  2.606e+05      2.24899   7.718e+02  5.214e+05
MPI Reductions:       8.532e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.2196e-01  54.9%  1.0573e+07 100.0%  5.495e+02  81.3%  7.718e+02      100.0%  2.270e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              86 1.0 2.1577e-04 1.2 3.67e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4730
MatLUFactorNum        86 1.0 2.9488e-03 1.2 3.43e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3229
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      86 1.0 9.4795e-04 1.2 0.00e+00 0.0 5.2e+02 1.0e+03 8.6e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        86 1.0 1.9207e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 9.3e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        86 1.0 1.9717e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               86 1.0 8.1992e-04 2.6 1.11e+04 1.2 0.0e+00 0.0e+00 4.3e+01  0  0  0  0  0   0  0  0  0 19    39
VecCopy               86 1.0 1.8597e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               174 1.0 2.8610e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          86 1.0 8.3566e-04 2.6 1.11e+04 1.2 0.0e+00 0.0e+00 4.3e+01  0  0  0  0  0   0  0  0  0 19    38
KSPSetUp              88 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              86 1.0 4.3423e-03 1.0 3.81e+06 1.2 0.0e+00 0.0e+00 4.3e+01  1100  0  0  0   2100  0  0 19  2435
PCSetUp              172 1.0 3.2218e-03 1.2 3.43e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2955
PCSetUpOnBlocks       86 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               86 1.0 3.4268e-03 1.2 3.72e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  3012
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.38554e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.168e-01      1.00000   4.168e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.901e+06      1.22124   3.606e+06  1.082e+07
Flops/sec:            9.360e+06      1.22124   8.652e+06  2.596e+07
MPI Messages:         3.110e+02      1.73743   2.303e+02  6.910e+02
MPI Message Lengths:  2.667e+05      2.24900   7.720e+02  5.335e+05
MPI Reductions:       8.735e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.2667e-01  54.4%  1.0819e+07 100.0%  5.620e+02  81.3%  7.720e+02      100.0%  2.320e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              88 1.0 2.2078e-04 1.2 3.75e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4730
MatLUFactorNum        88 1.0 3.0277e-03 1.2 3.51e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3218
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      88 1.0 9.7179e-04 1.2 0.00e+00 0.0 5.3e+02 1.0e+03 8.8e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        88 1.0 1.9698e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 9.5e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        88 1.0 2.0003e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               88 1.0 8.5187e-04 2.7 1.14e+04 1.2 0.0e+00 0.0e+00 4.4e+01  0  0  0  0  0   0  0  0  0 19    38
VecCopy               88 1.0 1.9550e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               178 1.0 2.8610e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          88 1.0 8.6761e-04 2.6 1.14e+04 1.2 0.0e+00 0.0e+00 4.4e+01  0  0  0  0  0   0  0  0  0 19    37
KSPSetUp              90 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              88 1.0 4.4472e-03 1.0 3.90e+06 1.2 0.0e+00 0.0e+00 4.4e+01  1100  0  0  0   2100  0  0 19  2433
PCSetUp              176 1.0 3.3047e-03 1.2 3.51e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2948
PCSetUpOnBlocks       88 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               88 1.0 3.5198e-03 1.2 3.81e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  3002
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.76566e-06
Average time for zero size MPI_Send(): 1.27157e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.287e-01      1.00000   4.287e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                3.990e+06      1.22124   3.688e+06  1.106e+07
Flops/sec:            9.307e+06      1.22125   8.603e+06  2.581e+07
MPI Messages:         3.180e+02      1.73770   2.355e+02  7.065e+02
MPI Message Lengths:  2.727e+05      2.24901   7.722e+02  5.456e+05
MPI Reductions:       8.938e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.3113e-01  53.9%  1.1065e+07 100.0%  5.745e+02  81.3%  7.722e+02      100.0%  2.370e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              90 1.0 2.2578e-04 1.2 3.84e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4730
MatLUFactorNum        90 1.0 3.1016e-03 1.2 3.59e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3212
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      90 1.0 9.9564e-04 1.2 0.00e+00 0.0 5.4e+02 1.0e+03 9.0e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        90 1.0 2.0149e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 9.7e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        90 1.0 2.0289e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               90 1.0 8.7309e-04 2.7 1.16e+04 1.2 0.0e+00 0.0e+00 4.5e+01  0  0  0  0  0   0  0  0  0 19    38
VecCopy               90 1.0 2.0266e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               182 1.0 2.9564e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          90 1.0 8.8882e-04 2.7 1.16e+04 1.2 0.0e+00 0.0e+00 4.5e+01  0  0  0  0  0   0  0  0  0 19    37
KSPSetUp              92 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              90 1.0 4.5464e-03 1.0 3.99e+06 1.2 0.0e+00 0.0e+00 4.5e+01  1100  0  0  0   2100  0  0 19  2434
PCSetUp              180 1.0 3.3820e-03 1.2 3.59e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2946
PCSetUpOnBlocks       90 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               90 1.0 3.6068e-03 1.2 3.90e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  2997
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.95639e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.403e-01      1.00000   4.403e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                4.079e+06      1.22124   3.770e+06  1.131e+07
Flops/sec:            9.263e+06      1.22125   8.563e+06  2.569e+07
MPI Messages:         3.250e+02      1.73797   2.407e+02  7.220e+02
MPI Message Lengths:  2.788e+05      2.24903   7.724e+02  5.577e+05
MPI Reductions:       9.141e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.3542e-01  53.5%  1.1311e+07 100.0%  5.870e+02  81.3%  7.724e+02      100.0%  2.420e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              92 1.0 2.3079e-04 1.2 3.92e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4730
MatLUFactorNum        92 1.0 3.1714e-03 1.2 3.67e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3211
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      92 1.0 1.0188e-03 1.2 0.00e+00 0.0 5.5e+02 1.0e+03 9.2e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        92 1.0 2.0587e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 9.9e+01  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        92 1.0 2.0504e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               92 1.0 8.9407e-04 2.7 1.19e+04 1.2 0.0e+00 0.0e+00 4.6e+01  0  0  0  0  0   0  0  0  0 19    38
VecCopy               92 1.0 2.0504e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               186 1.0 3.0518e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          92 1.0 9.0981e-04 2.7 1.19e+04 1.2 0.0e+00 0.0e+00 4.6e+01  0  0  0  0  0   0  0  0  0 19    37
KSPSetUp              94 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              92 1.0 4.6415e-03 1.0 4.08e+06 1.2 0.0e+00 0.0e+00 4.6e+01  1100  0  0  0   2100  0  0 19  2437
PCSetUp              184 1.0 3.4549e-03 1.2 3.67e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2948
PCSetUpOnBlocks       92 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               92 1.0 3.6900e-03 1.2 3.99e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  2996
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.19481e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.517e-01      1.00000   4.517e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                4.167e+06      1.22124   3.852e+06  1.156e+07
Flops/sec:            9.226e+06      1.22125   8.529e+06  2.559e+07
MPI Messages:         3.320e+02      1.73822   2.458e+02  7.375e+02
MPI Message Lengths:  2.848e+05      2.24904   7.726e+02  5.698e+05
MPI Reductions:       9.344e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.3954e-01  53.0%  1.1557e+07 100.0%  5.995e+02  81.3%  7.726e+02      100.0%  2.470e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              94 1.0 2.3460e-04 1.2 4.01e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4755
MatLUFactorNum        94 1.0 3.2384e-03 1.2 3.75e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3213
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      94 1.0 1.0419e-03 1.2 0.00e+00 0.0 5.6e+02 1.0e+03 9.4e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        94 1.0 2.1029e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 1.0e+02  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        94 1.0 2.0790e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               94 1.0 9.1910e-04 2.7 1.21e+04 1.2 0.0e+00 0.0e+00 4.7e+01  0  0  0  0  0   0  0  0  0 19    38
VecCopy               94 1.0 2.1458e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               190 1.0 3.1471e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          94 1.0 9.3484e-04 2.7 1.21e+04 1.2 0.0e+00 0.0e+00 4.7e+01  0  0  0  0  0   0  0  0  0 19    37
KSPSetUp              96 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              94 1.0 4.7386e-03 1.0 4.17e+06 1.2 0.0e+00 0.0e+00 4.7e+01  1100  0  0  0   2100  0  0 19  2439
PCSetUp              188 1.0 3.5260e-03 1.2 3.75e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2951
PCSetUpOnBlocks       94 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               94 1.0 3.7699e-03 1.2 4.08e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  2998
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.81334e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.628e-01      1.00000   4.628e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                4.256e+06      1.22124   3.934e+06  1.180e+07
Flops/sec:            9.197e+06      1.22125   8.502e+06  2.550e+07
MPI Messages:         3.390e+02      1.73846   2.510e+02  7.530e+02
MPI Message Lengths:  2.909e+05      2.24905   7.728e+02  5.819e+05
MPI Reductions:       9.548e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.4346e-01  52.6%  1.1802e+07 100.0%  6.120e+02  81.3%  7.728e+02      100.0%  2.520e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              96 1.0 2.3961e-04 1.2 4.09e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4754
MatLUFactorNum        96 1.0 3.3076e-03 1.2 3.83e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3213
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      96 1.0 1.0648e-03 1.2 0.00e+00 0.0 5.8e+02 1.0e+03 9.6e+01  0  0 76100  0   0  0 94100 38     0
MatAssemblyEnd        96 1.0 2.1439e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 1.0e+02  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        96 1.0 2.1076e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               96 1.0 9.3317e-04 2.7 1.24e+04 1.2 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  0   0  0  0  0 19    38
VecCopy               96 1.0 2.1458e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               194 1.0 3.3617e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          96 1.0 9.4986e-04 2.7 1.24e+04 1.2 0.0e+00 0.0e+00 4.8e+01  0  0  0  0  0   0  0  0  0 19    37
KSPSetUp              98 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              96 1.0 4.8296e-03 1.0 4.26e+06 1.2 0.0e+00 0.0e+00 4.8e+01  1100  0  0  0   2100  0  0 19  2444
PCSetUp              192 1.0 3.5961e-03 1.2 3.83e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2955
PCSetUpOnBlocks       96 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               96 1.0 3.8500e-03 1.2 4.16e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  2999
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.00407e-06
Average time for zero size MPI_Send(): 1.27157e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.736e-01      1.00000   4.736e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                4.345e+06      1.22124   4.016e+06  1.205e+07
Flops/sec:            9.174e+06      1.22124   8.480e+06  2.544e+07
MPI Messages:         3.460e+02      1.73869   2.562e+02  7.685e+02
MPI Message Lengths:  2.969e+05      2.24906   7.730e+02  5.940e+05
MPI Reductions:       9.751e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.4724e-01  52.2%  1.2048e+07 100.0%  6.245e+02  81.3%  7.730e+02      100.0%  2.570e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve              98 1.0 2.4366e-04 1.2 4.18e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4773
MatLUFactorNum        98 1.0 3.3715e-03 1.2 3.91e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3218
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      98 1.0 1.0898e-03 1.2 0.00e+00 0.0 5.9e+02 1.0e+03 9.8e+01  0  0 77100  0   0  0 94100 38     0
MatAssemblyEnd        98 1.0 2.1846e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 1.0e+02  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        98 1.0 2.1267e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               98 1.0 9.4628e-04 2.7 1.26e+04 1.2 0.0e+00 0.0e+00 4.9e+01  0  0  0  0  0   0  0  0  0 19    38
VecCopy               98 1.0 2.2411e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               198 1.0 3.3617e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          98 1.0 9.6297e-04 2.7 1.26e+04 1.2 0.0e+00 0.0e+00 4.9e+01  0  0  0  0  0   0  0  0  0 19    38
KSPSetUp             100 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              98 1.0 4.9167e-03 1.0 4.34e+06 1.2 0.0e+00 0.0e+00 4.9e+01  1100  0  0  0   2100  0  0 19  2451
PCSetUp              196 1.0 3.6640e-03 1.2 3.91e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2961
PCSetUpOnBlocks       98 1.0 1.9431e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1139
PCApply               98 1.0 3.9260e-03 1.2 4.25e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  3003
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.57492e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Tue Jan 10 22:55:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.842e-01      1.00000   4.842e-01
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                4.433e+06      1.22124   4.098e+06  1.229e+07
Flops/sec:            9.157e+06      1.22125   8.464e+06  2.539e+07
MPI Messages:         3.530e+02      1.73892   2.613e+02  7.840e+02
MPI Message Lengths:  3.030e+05      2.24908   7.732e+02  6.062e+05
MPI Reductions:       9.954e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.5085e-01  51.8%  1.2294e+07 100.0%  6.370e+02  81.2%  7.732e+02      100.0%  2.620e+02   0.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve             100 1.0 2.4748e-04 1.2 4.26e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0  4795
MatLUFactorNum       100 1.0 3.4325e-03 1.2 3.99e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  3225
MatILUFactorSym        2 1.0 6.1989e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin     100 1.0 1.1117e-03 1.2 0.00e+00 0.0 6.0e+02 1.0e+03 1.0e+02  0  0 77100  0   0  0 94100 38     0
MatAssemblyEnd       100 1.0 2.2244e-03 1.1 0.00e+00 0.0 1.2e+01 3.8e+01 1.1e+02  0  0  2  0  0   1  0  2  0 41     0
MatGetRowIJ            2 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.3127e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       100 1.0 2.1482e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm              100 1.0 9.5940e-04 2.7 1.29e+04 1.2 0.0e+00 0.0e+00 5.0e+01  0  0  0  0  0   0  0  0  0 19    38
VecCopy              100 1.0 2.3603e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               202 1.0 3.3617e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         100 1.0 9.7704e-04 2.7 1.29e+04 1.2 0.0e+00 0.0e+00 5.0e+01  0  0  0  0  0   0  0  0  0 19    38
KSPSetUp             102 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             100 1.0 5.0027e-03 1.0 4.43e+06 1.2 0.0e+00 0.0e+00 5.0e+01  1100  0  0  0   2100  0  0 19  2458
PCSetUp              200 1.0 3.7270e-03 1.2 3.99e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 90  0  0  0   1 90  0  0  0  2970
PCSetUpOnBlocks      100 1.0 1.9526e-04 1.1 7.99e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1134
PCApply              100 1.0 3.9990e-03 1.2 4.34e+06 1.2 0.0e+00 0.0e+00 0.0e+00  1 98  0  0  0   1 98  0  0  0  3010
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4         3348     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.6226e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Wed Nov 30 20:50:21 2016 on cml01 
Machine characteristics: Linux-4.2.0-42-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

